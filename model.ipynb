{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "app_train = pd.read_csv('application_train.csv')\n",
    "app_test = pd.read_csv('application_test.csv')\n",
    "bureau = pd.read_csv('bureau.csv')\n",
    "bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "pos_cash_balance = pd.read_csv('POS_CASH_balance.csv')\n",
    "credit_card_balance = pd.read_csv('credit_card_balance.csv')\n",
    "previous_application = pd.read_csv('previous_application.csv')\n",
    "installments_payments = pd.read_csv('installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 123)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282686"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imbalance\n",
    "app_train['TARGET'].shape[0] - np.sum(app_train['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LIVINGAREA_MODE',\n",
       " 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
       " 'ELEVATORS_MODE',\n",
       " 'DAYS_EMPLOYED',\n",
       " 'EXT_SOURCE_3',\n",
       " 'FLOORSMIN_MODE',\n",
       " 'YEARS_BEGINEXPLUATATION_MODE',\n",
       " 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
       " 'ELEVATORS_MEDI',\n",
       " 'REG_REGION_NOT_LIVE_REGION',\n",
       " 'COMMONAREA_MEDI',\n",
       " 'OWN_CAR_AGE',\n",
       " 'NONLIVINGAREA_MEDI',\n",
       " 'AMT_GOODS_PRICE',\n",
       " 'ENTRANCES_AVG',\n",
       " 'AMT_REQ_CREDIT_BUREAU_DAY',\n",
       " 'ELEVATORS_AVG',\n",
       " 'LIVINGAREA_AVG',\n",
       " 'LANDAREA_AVG',\n",
       " 'AMT_ANNUITY',\n",
       " 'LIVINGAPARTMENTS_MEDI',\n",
       " 'YEARS_BUILD_MEDI',\n",
       " 'TOTALAREA_MODE',\n",
       " 'LANDAREA_MEDI',\n",
       " 'EXT_SOURCE_2',\n",
       " 'NONLIVINGAPARTMENTS_MODE',\n",
       " 'HOUR_APPR_PROCESS_START',\n",
       " 'LIVINGAPARTMENTS_MODE',\n",
       " 'AMT_REQ_CREDIT_BUREAU_MON',\n",
       " 'FLOORSMIN_AVG',\n",
       " 'ENTRANCES_MODE',\n",
       " 'YEARS_BEGINEXPLUATATION_AVG',\n",
       " 'EXT_SOURCE_1',\n",
       " 'NONLIVINGAREA_AVG',\n",
       " 'REG_CITY_NOT_WORK_CITY',\n",
       " 'DAYS_LAST_PHONE_CHANGE',\n",
       " 'COMMONAREA_AVG',\n",
       " 'YEARS_BEGINEXPLUATATION_MEDI',\n",
       " 'CNT_CHILDREN',\n",
       " 'APARTMENTS_MEDI',\n",
       " 'REGION_POPULATION_RELATIVE',\n",
       " 'DAYS_BIRTH',\n",
       " 'FLOORSMAX_AVG',\n",
       " 'LIVE_CITY_NOT_WORK_CITY',\n",
       " 'FLOORSMAX_MODE',\n",
       " 'REGION_RATING_CLIENT',\n",
       " 'OBS_30_CNT_SOCIAL_CIRCLE',\n",
       " 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
       " 'AMT_INCOME_TOTAL',\n",
       " 'APARTMENTS_AVG',\n",
       " 'APARTMENTS_MODE',\n",
       " 'LANDAREA_MODE',\n",
       " 'NONLIVINGAPARTMENTS_AVG',\n",
       " 'FLOORSMAX_MEDI',\n",
       " 'DEF_30_CNT_SOCIAL_CIRCLE',\n",
       " 'ENTRANCES_MEDI',\n",
       " 'YEARS_BUILD_AVG',\n",
       " 'REG_CITY_NOT_LIVE_CITY',\n",
       " 'BASEMENTAREA_MODE',\n",
       " 'DAYS_ID_PUBLISH',\n",
       " 'BASEMENTAREA_AVG',\n",
       " 'NONLIVINGAREA_MODE',\n",
       " 'NONLIVINGAPARTMENTS_MEDI',\n",
       " 'DAYS_REGISTRATION',\n",
       " 'LIVINGAPARTMENTS_AVG',\n",
       " 'YEARS_BUILD_MODE',\n",
       " 'REGION_RATING_CLIENT_W_CITY',\n",
       " 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
       " 'FLOORSMIN_MEDI',\n",
       " 'LIVINGAREA_MEDI',\n",
       " 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
       " 'COMMONAREA_MODE',\n",
       " 'BASEMENTAREA_MEDI']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-process\n",
    "all_numerical_cols = list(app_train.select_dtypes(exclude='object').columns)\n",
    "cont_cols = [col for col in all_numerical_cols if col != \"TARGET\" and col[:5]!='FLAG_']\n",
    "\n",
    "app_train = app_train.drop(columns=['CNT_FAM_MEMBERS','LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_WORK_REGION', 'OBS_60_CNT_SOCIAL_CIRCLE'])\n",
    "app_test = app_test.drop(columns=['CNT_FAM_MEMBERS','LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_WORK_REGION', 'OBS_60_CNT_SOCIAL_CIRCLE'])\n",
    "cols_to_remove = ['AMT_CREDIT', 'CNT_FAM_MEMBERS', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'OBS_60_CNT_SOCIAL_CIRCLE','SK_ID_CURR']\n",
    "cont_cols = list(set(cont_cols) - set(cols_to_remove))\n",
    "cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app_train['LTV'] = app_train['AMT_CREDIT']/app_train['AMT_GOODS_PRICE']\n",
    "app_train['DTI'] = app_train['AMT_ANNUITY']/app_train['AMT_INCOME_TOTAL']\n",
    "app_train['Employed/Birth'] = app_train['DAYS_EMPLOYED']/app_train['DAYS_BIRTH'] \n",
    "app_train['Flag_Greater_30'] = (app_train['DAYS_BIRTH']/-365.25).apply(lambda x: 1 if x > 30 else 0)\n",
    "app_train['Flag_Employment_Greater_5'] = (app_train['DAYS_EMPLOYED']/-365.25).apply(lambda x: 1 if x > 5 else 0)\n",
    "\n",
    "# #用不上的\n",
    "# app_test['LTV'] = app_test['AMT_CREDIT']/app_test['AMT_GOODS_PRICE']\n",
    "# app_test['DTI'] = app_test['AMT_ANNUITY']/app_test['AMT_INCOME_TOTAL']\n",
    "# app_test['Employed/Birth'] = app_test['DAYS_EMPLOYED']/app_test['DAYS_BIRTH']\n",
    "# app_test['Flag_Greater_30'] = (app_test['DAYS_BIRTH']/-365.25).apply(lambda x: 1 if x > 30 else 0)\n",
    "# app_test['Flag_Employment_Greater_5'] = (app_test['DAYS_EMPLOYED']/-365.25).apply(lambda x: 1 if x > 5 else 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from category_encoders import TargetEncoder \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "#set X1 and Y1 and try to train\n",
    "cat_col = app_train.select_dtypes('object')\n",
    "enc = TargetEncoder()\n",
    "app_train[cat_col.columns] = enc.fit_transform(app_train[cat_col.columns], app_train['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 123)\n"
     ]
    }
   ],
   "source": [
    "#fill na\n",
    "k = 0\n",
    "for i in cat_col:\n",
    "    app_train[i].fillna(0, inplace=True)\n",
    "for i in cont_cols:\n",
    "    app_train[i].fillna(app_train[i].mean(), inplace=True)\n",
    "    sc = StandardScaler().fit(app_train[[i]])\n",
    "    app_train[i] = sc.transform(app_train[[i]])\n",
    "for i in app_train.columns:\n",
    "    if max(app_train[i])>5 and i not in cat_col:\n",
    "        sc = StandardScaler().fit(app_train[[i]])\n",
    "        app_train[i] = sc.transform(app_train[[i]])\n",
    "print(app_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189839, 123) (69927, 123)\n",
      "(538685, 121) (198509, 121)\n",
      "(47457, 121)\n"
     ]
    }
   ],
   "source": [
    "def posi_extend(X, y):\n",
    "    more_posi = (int) ((y.shape[0] - np.sum(y))*2)\n",
    "    posi_idx = [i for i in range(len(y)) if y.iloc[i] == 1]\n",
    "    indices = np.random.choice(posi_idx, size=more_posi, replace=True)\n",
    "    X_ex = X.iloc[indices]\n",
    "    y_ex = y.iloc[indices]\n",
    "    X = pd.concat([X, X_ex], axis=0)\n",
    "    y = pd.concat([y, y_ex], axis=0)\n",
    "    return X, y\n",
    "\n",
    "train1 = app_train.loc[:190000,:].dropna(axis=0)\n",
    "mask = np.isinf(train1)\n",
    "train1[mask] = np.nan\n",
    "train1 = train1.dropna(axis=0)\n",
    "\n",
    "train2 = app_train.loc[190000:260000,:].dropna(axis=0)\n",
    "mask = np.isinf(train2)\n",
    "train2[mask] = np.nan\n",
    "train2 = train2.dropna(axis=0)\n",
    "print(train1.shape, train2.shape)\n",
    "\n",
    "Y1 = train1.iloc[:,1]\n",
    "X1 = train1.drop([\"TARGET\", \"SK_ID_CURR\"],axis=1)\n",
    "Y2 = train2.iloc[:,1]\n",
    "X2 = train2.drop([\"TARGET\", \"SK_ID_CURR\"],axis=1)\n",
    "\n",
    "X1, Y1 = posi_extend(X1, Y1)\n",
    "X2, Y2 = posi_extend(X2, Y2)\n",
    "# print(app_test.head())\n",
    "# test = app_test.loc[:,vs].dropna(axis=0)\n",
    "# print(test.head(10))\n",
    "# Y_test = test.iloc[:,0]\n",
    "# X_test = test.drop([\"TARGET\"],axis=1)\n",
    "# print(X.shape, X_test.shape)\n",
    "print(X1.shape, X2.shape)\n",
    "\n",
    "#final test set, imbalanced\n",
    "test_data = app_train.loc[260000:,:].dropna(axis=0)\n",
    "YTest = test_data.iloc[:,1]\n",
    "XTest = test_data.drop([\"TARGET\", \"SK_ID_CURR\"],axis=1)\n",
    "print(XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377079, 121)\n",
      "done fit\n",
      "0.83388966445565 0.8854320332830551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#m1\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=15) #0.63\n",
    "#clf = RandomForestClassifier(random_state=42, max_depth=15) #40k imbalanced test: 0.7282299442204214 0.2274404834326966\n",
    "#clf = MLPClassifier(max_iter=100, hidden_layer_sizes=(512,128,32,32,32,16),alpha=0.1,learning_rate=0.001) #imbalanced test: AUC 0.662085296871909, F1 0.216791319919941\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"done fit\")\n",
    "# Predict using the testing data\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "# #m2\n",
    "# # Create a neural network classifier object\n",
    "# nn = MLPClassifier(hidden_layer_sizes=(50, ), max_iter=100)\n",
    "\n",
    "# # Fit the classifier to the training data\n",
    "# nn.fit(X_train, y_train)\n",
    "\n",
    "# # Predict using the testing data\n",
    "# y_pred_nn = nn.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "# f1 = f1_score(y_test, y_pred[:,1])\n",
    "auc = roc_auc_score(y_test, y_pred[:,1])\n",
    "# auc_nn = roc_auc_score(y_test, y_pred_nn)\n",
    "print(auc, f1_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6616511219947462 0.1979995619478718\n",
      "[[ 27120  25210]\n",
      " [  2438 106838]]\n",
      "[[22775 20930]\n",
      " [ 1040  2712]]\n"
     ]
    }
   ],
   "source": [
    "#On imbalanced raw 60k test set (no augmentation)\n",
    "print(roc_auc_score(YTest, clf.predict_proba(XTest)[:,1]), f1_score(YTest, clf.predict(XTest)))\n",
    "#on balanced validation set and on imbalanced test set\n",
    "print(confusion_matrix(y_test,  clf.predict(X_test)))\n",
    "print(confusion_matrix(YTest, clf.predict(XTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #models weight learner\n",
    "# import numpy as np\n",
    "\n",
    "# def linear_regression(X1, Y1, X, y, models, X_test, y_test, alpha=0.01, num_iterations=1000):\n",
    "    \n",
    "#     y_hats = []\n",
    "#     #models train on predictions\n",
    "#     ms = []\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.3, random_state=42)\n",
    "#     for i in range(len(models)):\n",
    "#         m = models[i]\n",
    "#         print(m)\n",
    "#         m.fit(X_train, y_train)\n",
    "#         y_hat = m.predict(X)\n",
    "#         y_hats.append(y_hat)\n",
    "#         ms.append(m)\n",
    "#     y_hats = np.array(y_hats)\n",
    "#     print(np.sum(y_hats[0]), np.sum(y_hats[1]))\n",
    "    \n",
    " \n",
    "#     X = y_hats.T \n",
    "#     print(\"after transpose, the input shape is: \" + str(X.shape))\n",
    "#     X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "#     print(\"X's value after stacking constant:\")\n",
    "#     print(X)\n",
    "#     theta = np.zeros(X.shape[1])\n",
    "\n",
    "#     # Define the cost function\n",
    "#     def compute_cost(X, y, theta):\n",
    "#         m = len(y)\n",
    "#         # X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "#         z = X.dot(theta)\n",
    "#         h = 1 / (1 + np.exp(-z))\n",
    "\n",
    "#         # Compute the cost function\n",
    "#         J = -1 / len(y) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "#         return J\n",
    "\n",
    "#     # Define the gradient descent function\n",
    "#     def gradient_descent(X, y, theta, alpha, num_iterations):\n",
    "#         m = len(y)\n",
    "#         J_history = np.zeros(num_iterations)\n",
    "#         for i in range(num_iterations):\n",
    "#             h = X.dot(theta)\n",
    "#             theta = theta - alpha*(1/m)*X.T.dot(h-y)\n",
    "#             J_history[i] = compute_cost(X, y, theta)\n",
    "#         return theta\n",
    "    \n",
    "#     def lr_predict(input, theta, ms):\n",
    "#         #input原始数据\n",
    "#         y_hats = []\n",
    "#         for m in ms:\n",
    "#             y_hats.append(m.predict(input))\n",
    "#         y_hats = np.array(y_hats)\n",
    "#         print(y_hats.shape)\n",
    "#         X = y_hats.T \n",
    "#         X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "#         print(X.shape)\n",
    "#         print(theta.shape)\n",
    "#         # Compute the hypothesis\n",
    "#         z = X.dot(theta)\n",
    "#         h = 1 / (1 + np.exp(-z))\n",
    "#         # Threshold the probabilities to get the predictions\n",
    "#         y_pred = (h > 0.5).astype(int)\n",
    "#         return y_pred.ravel()\n",
    "\n",
    "#     # Run gradient descent\n",
    "#     theta_star = gradient_descent(X, y, theta, alpha, num_iterations)\n",
    "\n",
    "#     # Predict using the testing data\n",
    "#     y_pred = lr_predict(X_test, theta_star, ms)\n",
    "    \n",
    "#     return accuracy_score(y_test,y_pred), f1_score(y_test,y_pred), roc_auc_score(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n",
      "MLPClassifier(hidden_layer_sizes=(50,), max_iter=100)\n",
      "4493 3\n",
      "after transpose, the input shape is: (75174, 2)\n",
      "X's value after stacking constant:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "(2, 59949)\n",
      "(59949, 3)\n",
      "(3,)\n",
      "(0.08073529166458156, 0.14940807853184954, 0.5)\n"
     ]
    }
   ],
   "source": [
    "# #model weights learner\n",
    "# models = [DecisionTreeClassifier(random_state=42), MLPClassifier(hidden_layer_sizes=(50, ), max_iter=100)]\n",
    "# X2 = train2.drop([\"TARGET\"],axis=1)\n",
    "# Y2 = train2.iloc[:,0]\n",
    "# X2_train, X2_test, y2_train, y2_test = train_test_split(X2,Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "# score = linear_regression(X, Y, X2_train, y2_train, models, X2_test, y2_test)\n",
    "# print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def linear_regression(X1, Y1, X2, y2, models, XTest, YTest, alpha=0.01, num_iterations=1000):\n",
    "    \n",
    "    y_hats = []\n",
    "    #models train on predictions\n",
    "    ms = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.3, random_state=42)\n",
    "    for i in range(len(models)):\n",
    "        m = models[i]\n",
    "        print(m)\n",
    "        m.fit(X_train, y_train)\n",
    "        print(roc_auc_score(y2, m.predict_proba(X2)[:,1]))\n",
    "        y_hat = m.predict_proba(X2)[:,1]\n",
    "        \n",
    "        \n",
    "        y_hats.append(y_hat)\n",
    "        ms.append(m)\n",
    "    y_hats = np.array(y_hats)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(y_hats.T , y2)\n",
    "    #检查回归方式是不是有问题\n",
    "    print(lr.coef_)\n",
    "    print(np.sqrt(mean_squared_error(y2, lr.predict(y_hats.T))))\n",
    "\n",
    "    #pred\n",
    "    input = []\n",
    "    for m in ms:\n",
    "        input.append(m.predict_proba(XTest)[:,1])\n",
    "    input = np.array(input)\n",
    "    y_pred = lr.predict_proba(input.T)[:,1]\n",
    "    print(np.sum(lr.predict(input.T)))\n",
    "    print(np.sum(YTest))\n",
    "\n",
    "    return roc_auc_score(YTest, y_pred), f1_score(YTest, lr.predict(input.T)), accuracy_score(YTest,lr.predict(input.T)), confusion_matrix(YTest, lr.predict(input.T))\n",
    "\n",
    "def lasso_reg(X1, Y1, X2, y2, models, X_test, y_test):\n",
    "    y_hats = []\n",
    "    #models train on predictions\n",
    "    ms = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.3, random_state=42)\n",
    "    for i in range(len(models)):\n",
    "        m = models[i]\n",
    "        print(m)\n",
    "        m.fit(X_train, y_train)\n",
    "        y_hat = m.predict(X2)\n",
    "        print(np.sum(y_hat))\n",
    "        y_hats.append(y_hat)\n",
    "        ms.append(m)\n",
    "    y_hats = np.array(y_hats)\n",
    "    lr = Lasso(alpha=0.1)\n",
    "    lr.fit(y_hats.T , y2)\n",
    "\n",
    "    #pred\n",
    "    input = []\n",
    "    for m in ms:\n",
    "        input.append(m.predict(X_test))\n",
    "    input = np.array(input)\n",
    "    # z = lr.predict(input.T)\n",
    "    # h = 1 / (1 + np.exp(-z))\n",
    "    # y_pred = (h > 0.5).astype(int)\n",
    "    y_pred = lr.predict_proba(input.T)\n",
    "    return balanced_accuracy_score(y_test,y_pred), f1_score(y_test,y_pred), roc_auc_score(y_test, y_pred), confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42)\n",
      "1.0\n",
      "Perceptron(random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Perceptron' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m Y_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m/Users/tq/Downloads/r/other/y_test_binary.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m models \u001b[39m=\u001b[39m [DecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m),Perceptron(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m),GaussianNB()]\n\u001b[0;32m---> 19\u001b[0m score \u001b[39m=\u001b[39m linear_regression(X1, Y1, X2, Y2, models, X_test, y_test)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(score)\n\u001b[1;32m     21\u001b[0m \u001b[39m#(0.9655362897038515, 0.9585798816568047, 0.9655362897038514)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#DT (0.8940309958569894, 0.861764705882353, 0.8940309958569894)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#Percep (0.9617768912076109, 0.9515418502202644, 0.9617768912076108)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#NN (0.9699555009973915, 0.9644970414201184, 0.9699555009973914)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m#DT + P 0.9617768912076109, 0.9515418502202644, 0.9617768912076108)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[198], line 17\u001b[0m, in \u001b[0;36mlinear_regression\u001b[0;34m(X1, Y1, X2, y2, models, X_test, y_test, alpha, num_iterations)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(m)\n\u001b[1;32m     16\u001b[0m m\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 17\u001b[0m \u001b[39mprint\u001b[39m(roc_auc_score(y_train, m\u001b[39m.\u001b[39;49mpredict_proba(X_train)[:,\u001b[39m1\u001b[39m]))\n\u001b[1;32m     18\u001b[0m y_hat \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mpredict(X2)\n\u001b[1;32m     21\u001b[0m y_hats\u001b[39m.\u001b[39mappend(y_hat)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Perceptron' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Load the digits dataset\n",
    "other_train_X = pd.read_csv(\"/Users/tq/Downloads/r/other/train_binary.csv\")\n",
    "other_train_y = pd.read_csv(\"/Users/tq/Downloads/r/other/y_train_binary.csv\")\n",
    "sp = (int)(len(other_train_X)*0.7)\n",
    "X1 = other_train_X.iloc[:sp,:]\n",
    "Y1 = other_train_y.iloc[:sp,:]\n",
    "X2 = other_train_X.iloc[:sp,:]\n",
    "Y2 = other_train_y.iloc[:sp,:]\n",
    "\n",
    "X_test = pd.read_csv(\"/Users/tq/Downloads/r/other/test_binary.csv\")\n",
    "Y_test = pd.read_csv(\"/Users/tq/Downloads/r/other/y_test_binary.csv\")\n",
    "\n",
    "models = [DecisionTreeClassifier(random_state=42),Perceptron(random_state=42),GaussianNB()]\n",
    "score = linear_regression(X1, Y1, X2, Y2, models, X_test, y_test)\n",
    "print(score)\n",
    "#(0.9655362897038515, 0.9585798816568047, 0.9655362897038514)\n",
    "#DT (0.8940309958569894, 0.861764705882353, 0.8940309958569894)\n",
    "#Percep (0.9617768912076109, 0.9515418502202644, 0.9617768912076108)\n",
    "#NN (0.9699555009973915, 0.9644970414201184, 0.9699555009973914)\n",
    "#DT + P 0.9617768912076109, 0.9515418502202644, 0.9617768912076108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([29535, 32326]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of positives \n",
    "np.unique(Y2, return_counts=True) #Ok to go\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,Y2, test_size=0.3, random_state=42)\n",
    "np.unique(y2_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.5-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Library/Python/3.9/site-packages (from xgboost) (1.10.1)\n",
      "Requirement already satisfied: numpy in /Library/Python/3.9/site-packages (from xgboost) (1.24.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.5\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=5, random_state=42)\n",
      "0.7126845428802702\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "0.757446129595033\n",
      "DecisionTreeClassifier(max_depth=5, random_state=12)\n",
      "0.7126845428802702\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "0.757446129595033\n",
      "RandomForestClassifier(max_depth=15, random_state=42)\n",
      "0.7292158945581415\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "0.7374786477559022\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)\n",
      "0.7117321715841503\n",
      "DecisionTreeClassifier(max_depth=5, max_features=50, random_state=123)\n",
      "0.7064982500253187\n",
      "DecisionTreeClassifier(max_depth=5, max_features=50, min_samples_leaf=50,\n",
      "                       random_state=42)\n",
      "0.6991259497485264\n",
      "DecisionTreeClassifier(max_depth=5, min_samples_leaf=50, random_state=333)\n",
      "0.7126845428802702\n",
      "DecisionTreeClassifier(max_depth=15, random_state=42)\n",
      "0.6306006476301472\n",
      "DecisionTreeClassifier(max_depth=5, max_features=10, random_state=1)\n",
      "0.652178343101459\n",
      "DecisionTreeClassifier(max_depth=5, min_samples_leaf=100, random_state=12)\n",
      "0.7126845428802702\n",
      "DecisionTreeClassifier(max_depth=15, min_samples_leaf=50, random_state=42)\n",
      "0.6585479481793624\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=15, random_state=123)\n",
      "0.6349451799565847\n",
      "DecisionTreeClassifier(max_depth=5, random_state=42)\n",
      "0.7126845428802702\n",
      "DecisionTreeClassifier(criterion='log_loss', max_depth=5, random_state=42)\n",
      "0.7117321715841503\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)\n",
      "0.7117321715841503\n",
      "DecisionTreeClassifier(max_depth=10, random_state=42)\n",
      "0.6963911950798647\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)\n",
      "0.7117321715841503\n",
      "DecisionTreeClassifier(max_depth=15, min_samples_leaf=200, random_state=42)\n",
      "0.6954097921146424\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=123)\n",
      "0.7117321715841503\n",
      "DecisionTreeClassifier(max_depth=5, max_features=50, random_state=42)\n",
      "0.6991030148255829\n",
      "DecisionTreeClassifier(max_depth=5, min_samples_split=50, random_state=123)\n",
      "0.7126845428802702\n",
      "DecisionTreeClassifier(max_depth=5, max_features=10, random_state=42)\n",
      "0.663146585629844\n",
      "DecisionTreeClassifier(criterion='log_loss', max_depth=5, min_samples_split=10,\n",
      "                       random_state=42)\n",
      "0.7117321715841503\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=123)\n",
      "0.7117321715841503\n",
      "DecisionTreeClassifier(max_depth=5, min_samples_leaf=100, random_state=42)\n",
      "0.7126845428802702\n",
      "DecisionTreeClassifier(max_depth=5, random_state=42)\n",
      "0.7126845428802702\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7435189624777554\n",
      "MLPClassifier(hidden_layer_sizes=(20,))\n",
      "0.7373770013242968\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.74334136429996\n",
      "MLPClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6848982334429878\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7428590011058929\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.742271303826304\n",
      "MLPClassifier(hidden_layer_sizes=(5,))\n",
      "0.7481371492652447\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7468735401787696\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7426437344058363\n",
      "MLPClassifier(hidden_layer_sizes=(5, 5))\n",
      "0.7464214834340681\n",
      "MLPClassifier(hidden_layer_sizes=(10, 10, 1))\n",
      "0.739112185907542\n",
      "MLPClassifier(hidden_layer_sizes=(5, 10, 10, 1))\n",
      "0.7407038251393572\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7452051604429887\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7429389730477013\n",
      "MLPClassifier(hidden_layer_sizes=(5,))\n",
      "0.7461844804070284\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.743167009237893\n",
      "MLPClassifier(hidden_layer_sizes=(30,))\n",
      "0.7287523972667147\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7415399641020053\n",
      "MLPClassifier(hidden_layer_sizes=(30,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7276411837008769\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7440157595387369\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7428163657007336\n",
      "MLPClassifier(hidden_layer_sizes=(80,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6937879094940456\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7432178158266106\n",
      "MLPClassifier(hidden_layer_sizes=(200,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6634398470388071\n",
      "MLPClassifier(hidden_layer_sizes=(10,))\n",
      "0.7412447782886717\n",
      "[[ 0.02729774  1.39394647  0.02729774  1.39394647 -0.83746658  0.74584803\n",
      "   0.01177244 -0.54611626  0.08093013  0.02729774 -0.08324886  0.28641155\n",
      "   0.02729774 -0.0762537  -0.01481117  0.02729774  0.01177244  0.01177244\n",
      "   0.02064224  0.01177244  0.13539455  0.01177244  0.1033838   0.02729774\n",
      "  -0.06416864  0.01177244  0.01177244  0.02729774  0.02729774  0.0601474\n",
      "   0.21374377  0.27171509  0.02948591  0.17730095 -0.29776511  0.59630169\n",
      "   0.59157335  0.04217116  0.61357489  0.14995234 -0.02118433  0.3252643\n",
      "  -0.03419235 -0.12859908 -0.14447747  0.15821255 -0.23107029  0.12328013\n",
      "   0.35055679 -0.25487153  0.04130753  0.02699131  0.05383864 -0.66544115]]\n",
      "0.552372662494147\n",
      "17200\n",
      "3752\n",
      "(0.7583002583955376, 0.25496372661321115, 0.6710706534336347, array([[29176, 14529],\n",
      "       [ 1081,  2671]]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "models = [DecisionTreeClassifier(random_state=42,max_depth=5),\n",
    "          XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, seed=42),\n",
    "          DecisionTreeClassifier(random_state=12,max_depth=5),\n",
    "          XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, seed=123),\n",
    "          RandomForestClassifier(random_state=42, max_depth=15),\n",
    "          XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=100, seed=34),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,criterion='entropy'),\n",
    "          DecisionTreeClassifier(random_state=123,max_depth=5,max_features=50),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,max_features=50,min_samples_leaf=50),\n",
    "          DecisionTreeClassifier(random_state=333,max_depth=5,min_samples_leaf=50),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=15,criterion='gini'),\n",
    "          DecisionTreeClassifier(random_state=1,max_depth=5,max_features=10),\n",
    "          DecisionTreeClassifier(random_state=12,max_depth=5,min_samples_leaf=100),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=15,min_samples_leaf=50),\n",
    "          DecisionTreeClassifier(random_state=123,max_depth=15,criterion='entropy'),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,criterion='gini'),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,criterion='log_loss'),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,criterion='entropy'),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=10),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,criterion='entropy'),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=15,min_samples_leaf=200),\n",
    "          DecisionTreeClassifier(random_state=123,max_depth=5, criterion='entropy'),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,max_features=50),\n",
    "          DecisionTreeClassifier(random_state=123,max_depth=5,min_samples_split=50),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,max_features=10),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,criterion='log_loss', min_samples_split=10),\n",
    "          DecisionTreeClassifier(random_state=123,max_depth=5,criterion='entropy'),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5,min_samples_leaf=100),\n",
    "          DecisionTreeClassifier(random_state=42,max_depth=5),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(20,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(100,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(5,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(5,5)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,10,1)),\n",
    "          MLPClassifier(hidden_layer_sizes=(5,10,10,1)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(5,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(30,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(30,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(80,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(200,)),\n",
    "          MLPClassifier(hidden_layer_sizes=(10,)),\n",
    "]\n",
    "score = linear_regression(X1, Y1, X2, Y2, models, XTest, YTest)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47457,)\n"
     ]
    }
   ],
   "source": [
    "print(YTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>LTV</th>\n",
       "      <th>DTI</th>\n",
       "      <th>Employed/Birth</th>\n",
       "      <th>Flag_Greater_30</th>\n",
       "      <th>Flag_Employment_Greater_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.733423</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083459</td>\n",
       "      <td>0.101419</td>\n",
       "      <td>0.085002</td>\n",
       "      <td>0.079616</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>0.142129</td>\n",
       "      <td>-0.478095</td>\n",
       "      <td>-0.166149</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.795715e-02</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-3.138730e-01</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-5.176655e-01</td>\n",
       "      <td>0.285399</td>\n",
       "      <td>0.121978</td>\n",
       "      <td>0.067329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.733413</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083459</td>\n",
       "      <td>0.069993</td>\n",
       "      <td>0.085002</td>\n",
       "      <td>0.083249</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>0.426792</td>\n",
       "      <td>1.725450</td>\n",
       "      <td>0.592677</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.795715e-02</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-3.138730e-01</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-1.092866e+00</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.132217</td>\n",
       "      <td>0.070862</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.733403</td>\n",
       "      <td>0</td>\n",
       "      <td>0.054783</td>\n",
       "      <td>0.101419</td>\n",
       "      <td>0.072437</td>\n",
       "      <td>0.079616</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.427196</td>\n",
       "      <td>-1.152888</td>\n",
       "      <td>-1.404676</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.795715e-02</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-3.138730e-01</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-1.092866e+00</td>\n",
       "      <td>-0.991538</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.733384</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083459</td>\n",
       "      <td>0.069993</td>\n",
       "      <td>0.085002</td>\n",
       "      <td>0.079616</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.142533</td>\n",
       "      <td>-0.711430</td>\n",
       "      <td>0.177869</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.920631e-17</td>\n",
       "      <td>2.207782e-17</td>\n",
       "      <td>-1.501907e-17</td>\n",
       "      <td>-4.456258e-17</td>\n",
       "      <td>-5.172418e-17</td>\n",
       "      <td>-0.565861</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.159905</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.733374</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083459</td>\n",
       "      <td>0.101419</td>\n",
       "      <td>0.085002</td>\n",
       "      <td>0.079616</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.199466</td>\n",
       "      <td>-0.213734</td>\n",
       "      <td>-0.361755</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.795715e-02</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-3.138730e-01</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-1.092866e+00</td>\n",
       "      <td>-0.991538</td>\n",
       "      <td>0.179963</td>\n",
       "      <td>0.152418</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  \\\n",
       "0   -1.733423       1            0.083459     0.101419      0.085002   \n",
       "1   -1.733413       0            0.083459     0.069993      0.085002   \n",
       "2   -1.733403       0            0.054783     0.101419      0.072437   \n",
       "3   -1.733384       0            0.083459     0.069993      0.085002   \n",
       "4   -1.733374       0            0.083459     0.101419      0.085002   \n",
       "\n",
       "   FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0         0.079616     -0.577538          0.142129   -0.478095    -0.166149   \n",
       "1         0.083249     -0.577538          0.426792    1.725450     0.592677   \n",
       "2         0.079616     -0.577538         -0.427196   -1.152888    -1.404676   \n",
       "3         0.079616     -0.577538         -0.142533   -0.711430     0.177869   \n",
       "4         0.079616     -0.577538         -0.199466   -0.213734    -0.361755   \n",
       "\n",
       "   ...  AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "0  ...              -6.795715e-02               -1.805048e-01   \n",
       "1  ...              -6.795715e-02               -1.805048e-01   \n",
       "2  ...              -6.795715e-02               -1.805048e-01   \n",
       "3  ...              -2.920631e-17                2.207782e-17   \n",
       "4  ...              -6.795715e-02               -1.805048e-01   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0              -3.138730e-01              -3.594746e-01   \n",
       "1              -3.138730e-01              -3.594746e-01   \n",
       "2              -3.138730e-01              -3.594746e-01   \n",
       "3              -1.501907e-17              -4.456258e-17   \n",
       "4              -3.138730e-01              -3.594746e-01   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_YEAR       LTV       DTI  Employed/Birth  \\\n",
       "0               -5.176655e-01  0.285399  0.121978        0.067329   \n",
       "1               -1.092866e+00  0.179000  0.132217        0.070862   \n",
       "2               -1.092866e+00 -0.991538  0.100000        0.011814   \n",
       "3               -5.172418e-17 -0.565861  0.219900        0.159905   \n",
       "4               -1.092866e+00 -0.991538  0.179963        0.152418   \n",
       "\n",
       "   Flag_Greater_30  Flag_Employment_Greater_5  \n",
       "0                0                          0  \n",
       "1                1                          0  \n",
       "2                1                          0  \n",
       "3                1                          1  \n",
       "4                1                          1  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggregates previous.csv\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_PREV</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>NAME_CONTRACT_STATUS</th>\n",
       "      <th>DAYS_DECISION</th>\n",
       "      <th>DAYS_LAST_DUE</th>\n",
       "      <th>DAYS_TERMINATION</th>\n",
       "      <th>TOTAL_ANNUITY</th>\n",
       "      <th>TOTAL_APPLICATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1369693</td>\n",
       "      <td>100001</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>3951.0</td>\n",
       "      <td>23787.0</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-1740</td>\n",
       "      <td>-1619.0</td>\n",
       "      <td>-1612.0</td>\n",
       "      <td>3951.000</td>\n",
       "      <td>23787.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1038818</td>\n",
       "      <td>100002</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>9251.775</td>\n",
       "      <td>179055.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>9</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-606</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>9251.775</td>\n",
       "      <td>179055.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1810518</td>\n",
       "      <td>100003</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>98356.995</td>\n",
       "      <td>1035882.0</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-746</td>\n",
       "      <td>-536.0</td>\n",
       "      <td>-527.0</td>\n",
       "      <td>169661.970</td>\n",
       "      <td>1452573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1564014</td>\n",
       "      <td>100004</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>5357.25</td>\n",
       "      <td>20106.0</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>5</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-815</td>\n",
       "      <td>-724.0</td>\n",
       "      <td>-714.0</td>\n",
       "      <td>5357.250</td>\n",
       "      <td>20106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1857999</td>\n",
       "      <td>100005</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>-315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4813.200</td>\n",
       "      <td>40153.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2248017</td>\n",
       "      <td>456251</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>6605.91</td>\n",
       "      <td>40455.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>17</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-273</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>6605.910</td>\n",
       "      <td>40455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1503599</td>\n",
       "      <td>456252</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>10074.465</td>\n",
       "      <td>56821.5</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-2497</td>\n",
       "      <td>-2316.0</td>\n",
       "      <td>-2311.0</td>\n",
       "      <td>10074.465</td>\n",
       "      <td>56821.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1686207</td>\n",
       "      <td>456253</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>5567.715</td>\n",
       "      <td>27306.0</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-1909</td>\n",
       "      <td>-1716.0</td>\n",
       "      <td>-1712.0</td>\n",
       "      <td>9540.810</td>\n",
       "      <td>41251.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016407</td>\n",
       "      <td>456254</td>\n",
       "      <td>Consumer loans</td>\n",
       "      <td>19065.825</td>\n",
       "      <td>247423.5</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>Approved</td>\n",
       "      <td>-277</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>365243.0</td>\n",
       "      <td>21362.265</td>\n",
       "      <td>268879.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1296007</td>\n",
       "      <td>456255</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>30737.655</td>\n",
       "      <td>1067940.0</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>Refused</td>\n",
       "      <td>-171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166203.135</td>\n",
       "      <td>3395448.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338857 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_PREV SK_ID_CURR NAME_CONTRACT_TYPE AMT_ANNUITY AMT_CREDIT  \\\n",
       "0     1369693     100001     Consumer loans      3951.0    23787.0   \n",
       "0     1038818     100002     Consumer loans    9251.775   179055.0   \n",
       "0     1810518     100003         Cash loans   98356.995  1035882.0   \n",
       "0     1564014     100004     Consumer loans     5357.25    20106.0   \n",
       "0     1857999     100005         Cash loans         NaN        0.0   \n",
       "..        ...        ...                ...         ...        ...   \n",
       "0     2248017     456251     Consumer loans     6605.91    40455.0   \n",
       "0     1503599     456252     Consumer loans   10074.465    56821.5   \n",
       "0     1686207     456253     Consumer loans    5567.715    27306.0   \n",
       "0     2016407     456254     Consumer loans   19065.825   247423.5   \n",
       "0     1296007     456255         Cash loans   30737.655  1067940.0   \n",
       "\n",
       "   WEEKDAY_APPR_PROCESS_START HOUR_APPR_PROCESS_START NAME_CONTRACT_STATUS  \\\n",
       "0                      FRIDAY                      13             Approved   \n",
       "0                    SATURDAY                       9             Approved   \n",
       "0                      FRIDAY                      12             Approved   \n",
       "0                      FRIDAY                       5             Approved   \n",
       "0                      FRIDAY                      10             Canceled   \n",
       "..                        ...                     ...                  ...   \n",
       "0                    THURSDAY                      17             Approved   \n",
       "0                   WEDNESDAY                      10             Approved   \n",
       "0                    SATURDAY                      12             Approved   \n",
       "0                    SATURDAY                      18             Approved   \n",
       "0                      FRIDAY                      14              Refused   \n",
       "\n",
       "   DAYS_DECISION DAYS_LAST_DUE DAYS_TERMINATION  TOTAL_ANNUITY  \\\n",
       "0          -1740       -1619.0          -1612.0       3951.000   \n",
       "0           -606         -25.0            -17.0       9251.775   \n",
       "0           -746        -536.0           -527.0     169661.970   \n",
       "0           -815        -724.0           -714.0       5357.250   \n",
       "0           -315           NaN              NaN       4813.200   \n",
       "..           ...           ...              ...            ...   \n",
       "0           -273         -30.0            -25.0       6605.910   \n",
       "0          -2497       -2316.0          -2311.0      10074.465   \n",
       "0          -1909       -1716.0          -1712.0       9540.810   \n",
       "0           -277      365243.0         365243.0      21362.265   \n",
       "0           -171           NaN              NaN     166203.135   \n",
       "\n",
       "    TOTAL_APPLICATION  \n",
       "0             23787.0  \n",
       "0            179055.0  \n",
       "0           1452573.0  \n",
       "0             20106.0  \n",
       "0             40153.5  \n",
       "..                ...  \n",
       "0             40455.0  \n",
       "0             56821.5  \n",
       "0             41251.5  \n",
       "0            268879.5  \n",
       "0           3395448.0  \n",
       "\n",
       "[338857 rows x 13 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, May  7 2023, 23:32:44) \n[Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
